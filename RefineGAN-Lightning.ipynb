{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d7fe877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
    "\n",
    "import librosa\n",
    "import torchyin\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c618c038",
   "metadata": {},
   "source": [
    "### Pitch Estimation and Speech Template Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825b1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech_template(audio_file):\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(audio_file, mono=True)\n",
    "\n",
    "    # Compute pitch using torchyin library\n",
    "    pitch = torchyin.estimate(audio, sample_rate=sr)\n",
    "\n",
    "    # Compute mel spectrogram using torchaudio library\n",
    "    mel_spec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=2048, hop_length=256)\n",
    "    mel_spec = mel_spec_transform(torch.Tensor(audio)).numpy()\n",
    "    \n",
    "    speech_template = np.zeros_like(audio)\n",
    "    voiced_indices = np.where(pitch > 0)[0]\n",
    "    \n",
    "    for i in range(len(voiced_indices)):\n",
    "        idx = voiced_indices[i]\n",
    "        f0 = pitch[idx]\n",
    "        pulse_length = int(np.round(sr/f0))\n",
    "        pulse = np.zeros(pulse_length)\n",
    "        pulse[0] = 1\n",
    "        speech_template[idx:idx+pulse_length] += pulse\n",
    "\n",
    "\n",
    "    unvoiced_indices = np.where(pitch <= 0)[0]\n",
    "    for i in range(len(unvoiced_indices)):\n",
    "        idx = unvoiced_indices[i]\n",
    "        speech_template[idx] = np.random.uniform(-1, 1)\n",
    "\n",
    "    return speech_template, pitch, torch.tensor(mel_spec.reshape(1,128,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a06d212",
   "metadata": {},
   "outputs": [],
   "source": [
    "template, pitch, mel_spec = generate_speech_template('./reference.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eea515",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01897661",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, channels, kernel_size=1, dilation=(1, 3)):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.convs = nn.ModuleList([ \n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0])),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1]))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, 0.1)\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd454d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=1, mel_channels=128, output_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 16, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(16),\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv1d(16, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(32),\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(64),\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv1d(64, mel_channels, kernel_size=5, stride=2, padding=2),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "\n",
    "        self.padding = nn.ConstantPad1d((0, 1), 0)\n",
    "        \n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(mel_channels * 2, 64, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(64),\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(64 * 2, 32, kernel_size=5, stride=2, padding=2, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(32),\n",
    "        )\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(32 * 2, 16, kernel_size=3, stride=2, padding=2, output_padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            ResBlock(16),\n",
    "        )\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.ConvTranspose1d(16 * 2, output_channels, kernel_size=5, stride=2, padding=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, waveform, mel_spectrogram):\n",
    "        # Encoder\n",
    "        enc1_out = self.enc1(waveform)\n",
    "        enc2_out = self.enc2(enc1_out)\n",
    "        enc3_out = self.enc3(enc2_out)\n",
    "        enc4_out = self.enc4(enc3_out)\n",
    "\n",
    "        mel_spectrogram_resized = torch.nn.functional.interpolate(mel_spectrogram, size=enc4_out.shape[-1], mode='nearest')\n",
    "\n",
    "        enc4_out_cat = torch.cat((enc4_out, mel_spectrogram_resized), dim=1)\n",
    "\n",
    "        dec1_out = self.dec1(enc4_out_cat)\n",
    "        dec1_out_cat = torch.cat((dec1_out, enc3_out), dim=1)\n",
    "\n",
    "        dec2_out = self.dec2(dec1_out_cat)\n",
    "        dec2_out_cat = torch.cat((dec2_out, self.padding(enc2_out)), dim=1)\n",
    "\n",
    "        dec3_out = self.dec3(dec2_out_cat)\n",
    "        dec3_out_cat = torch.cat((dec3_out, self.padding(enc1_out)), dim=1)\n",
    "\n",
    "        output = self.dec4(dec3_out_cat)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a532e03",
   "metadata": {},
   "source": [
    "## Defining Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6e31e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_loss(ref_mel, gen_mel):\n",
    "    return torch.nn.functional.mse_loss(ref_mel, gen_mel, reduction='mean')\n",
    "\n",
    "def envelope_loss(ref, gen):\n",
    "    m = nn.MaxPool1d(5, stride=3)\n",
    "    mae = nn.L1Loss()\n",
    "    \n",
    "    return mae(m(ref), m(gen)) + mae(m(-ref), m(-gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541b4b0",
   "metadata": {},
   "source": [
    "Training With Pytorch Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f8a61c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, unet):\n",
    "        super(LightningModel, self).__init__()\n",
    "        self.unet = unet\n",
    "        self.sl = spectrogram_loss\n",
    "        self.el = envelope_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, mel, y = batch\n",
    "        output = self.unet(x, mel)\n",
    "        loss1 = self.sl(y, x)\n",
    "        loss2 = self.el(y, x)\n",
    "#         loss = nn.functional.mse_loss(x, y)\n",
    "        return loss1+loss2\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "lightningmodel = LightningModel(UNet())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2dace79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, template, mel, ref):\n",
    "        self.template = torch.tensor(template.reshape(1,-1))\n",
    "        self.mel = torch.tensor(mel.reshape(128,-1))\n",
    "        self.ref = torch.tensor(ref.reshape(1,-1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return Variable(self.template,requires_grad=True), self.mel, self.ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ab76797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26873/1285992597.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mel = torch.tensor(mel.reshape(128,-1))\n",
      "/tmp/ipykernel_26873/1285992597.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ref = torch.tensor(ref.reshape(1,-1))\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "audio_file = \"./reference.wav\"\n",
    "mel_channels = 128\n",
    "waveform, sample_rate = torchaudio.load(audio_file)\n",
    "waveform = waveform.unsqueeze(0) \n",
    "\n",
    "ds = CustomDataset(template, mel_spec, waveform)\n",
    "train_dataloader = DataLoader(ds, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "909b94b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3080') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | unet | UNet | 182 K \n",
      "------------------------------\n",
      "182 K     Trainable params\n",
      "0         Non-trainable params\n",
      "182 K     Total params\n",
      "0.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348939345a434b7b8f6b09dc3162f077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(limit_train_batches=1, max_epochs=1)\n",
    "trainer.fit(model=lightningmodel, train_dataloaders=train_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
